* What's in a search engine?
** Building the collection
- 'crawling': starting with small set of documents, and following
  links
- Or might already have a collection
** Indexing
- big table of documents, locations of different words within them
- some way of getting to the document from its entry in the table (a
  link, or perhaps storing document itself in the table)
** Extracting ranked list from query
- Simple to extract documents with a given set of words, given an
  index
- Challenge is to extract most *relevent* documents.  How the results
  are sorted.
* urllib2
- standard python library
- "makes it easy to download pages - just provide URL"
* SQLite:
- "an embedded database that is very easy to set up, and stores a
  whole database in one file."
- SQL for queries, so code can be easily ported to use a different
  database.
- Python implementation: pysqlite
* Ranking:
- Easy to pull documents containing given words from the database (or
  more generally, matching some query); trick to good quality search
  is ranking the results.
- Give an item a "score" for its relevance to a given query; then
  return the highest ranking scores first. (but not necessarily
  globally sorted?  Just need to give "good enough"?)
** Content-based ranking
- ranking based solely on the content of a document
- Examples:
  - word frequency: If a doc uses one of the query words very
    frequently, it's likely to be relevant
  - Document location: If the query term appears e.g. near the
    beginning of a document, it's likely to be relevant
  - Word distance:  If a query contains multiple terms, then a
    document is more likely to be relevant if those terms appear close
    together in it.
- Normalise the scores to give a number between 0 and 1; 1 being the
  highest possible.
- Can use several score measures, and then combine them in some
  weighted average (or some other more complex combining procedure)
** "Network" ranking (not sure what the appropriate term is...)
- Trying to rank documents by their position in a larger network of
  documents.
- Inbound links:
  - "A page's importance can be estimated by the number of links to it
    from other pages"
- PageRank algorithm:
  - "A page's importance can be estimated by the number of links it
    receives from other important pages."
  - Apparently equal to the probability that someone randomly clicking
    links will arrive at a certain page.
- Incorporating link text:  (kind of "link valence")
  - "will get better information from what the links to a page say
    about it, than from the linking page itself, as site developers
    tend to include a short description of whatever it is they are
    linking to."
  - IDEA: Could incorporate this into dealing with academic articles
    by looking at the text preceding a reference to an article? How
    could one automatically pull such text out of a PDF?
    Alternatively, perhaps could start with PLoS and other open access
    journals?  Similarly, html versions of articles?  How would you
    design the "crawler" for that process?  In many ways, I'm most
    interested in indexing and searching my personal library of
    articles; perhaps finding articles that I don't have but should
    --- that could be a cool use, actually: given a bunch of articles,
    find the "closure" (for some appropriate definition of closure...)
* Learning from users
- "One of the major advantages of online applications is that they
  receive constant feedback in the form of user behaviour"
  - which link does a user choose to click on?
* Writing the code:
** crawler object:
- Q: How do you write unit tests for something like crawler.crawl?  Is
  this where a "mock object" comes in?  i.e. instead of requesting
  pages from the web, the crawler requests them from some other
  object?  Or is this an example of where unit testing would force you
  to create smaller modules of behaviour just so you can test?
  (i.e. a function that extracts links from a page; not sure what else
  though...)
** Toby Segaran's searchindex.db; weird behaviour?
- pg 62: suggests downloading his searchindex.db, loading it up and testing it using:
  [row for rowid in crawler.db.execute('select rowid from wordlocation where wordid=1')]
- In theory, should give some particular result (given in the book)
- I get a different result; trying to trace down the issue, I first of
  all looked at what word was associated with rowid=1 --> I got
  'doctype'; Toby Segaran mentions that it should be "word"
- And indeed, when I pull out the rowid for 'word', I get 1.
- But I also get 2, 3, 4, ... --- in other words, matching
  'word="word"' pulls out everything in the table!  WHY?!
- One hypothesis is that it's because word is also the name of the
  column... in which case I should get the same result doing
  word='rowid'?  Nope: get nothing
- what about rowid='rowid'? YES: in that case, I pull out everything.
  Similarly, if I try different kinds of capitalisation for the entry;
  i.e. rowid="RoWiD" etc.
- This capitalisation thing doesn't do anything if you're not trying
  to match the column name as the value.
- AHAH: it is dependent on whether you use single or double quotes in
  the SQL string --- single quotes work, but double quotes seem to
  treat the query as matching entries or column names.  Very strange
  behaviour if you ask me... :-/
- Now I get it: in sql, double quotes surround entire sql statements,
  so word="word" was like saying "pull out everything that matches
  anything in the word column"!
- So now I need to go back and fix up the stuff in searchengine.py, to
  make sure I don't use double quotes!
- Though note: while this explains why I was getting weird results
  when trying to work out why Toby's db doesn't give the results
  listed in his book, it doesn't explain why Toby's db doesn't give
  the results in his book!  Maybe the db has changed?
** Refactoring: Split database handling nito a separate object
- It seems to me that the code for this chapter could be improved by
  splitting out the database interaction code into a different object,
  leaving the crawler code to just do the actual "crawling"
  functionality, and the searcher code to focus on ranking and
  supporting higher level search commands.
- I *think* this would be an example of "separation of concerns" and
  the principal that each object should do "just one thing".
- It would also make things easier to test, as I could test the db
  code separately, and create a mock database for testing the
  searcher.
- Disadvantage might be that I'd need to think through the higher
  level db operations I'd want to support; otherwise, why not just use
  SQL?
** Current strategy for querying doesn't scale very well
- The strategy for querying taken by Toby Segaran involves first
  constructing a tuple for each possible combination of term positions
  in each url containing all the terms (i.e. using
  searcher.getmatchrows()).
- This leads to a combinatorial explosion when searching for e.g. a
  combination of words that tend to occur together many times in a
  document.  For example, "functional programming" produces 13786 such
  combinations, but "functional programming haskell" produces 96023,
  and takes a long time!
- To get around this, perhaps the best way would be first to pull out
  the list of all pages containing all the terms, and then for each
  page pull out the list of word locations.
- This then pushes the efficiency issue onto the scoring code (though
  presumably some of that code could be made much more efficient using
  this method).
